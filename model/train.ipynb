{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, callbacks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1fa9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data\n",
    "processed_data = pd.read_csv('../data/processed_financial_data.csv')\n",
    "with open('../model/feature_info.json', 'r') as f:\n",
    "    feature_info = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb53dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialAttentionModel:\n",
    "    def __init__(self, n_features, n_heads=8, d_model=128, dropout_rate=0.1):\n",
    "        self.n_features = n_features\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def positional_encoding(self, seq_len, d_model):\n",
    "        \"\"\"Create positional encoding for sequence data\"\"\"\n",
    "        position = np.arange(seq_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        \n",
    "        pos_encoding = np.zeros((seq_len, d_model))\n",
    "        pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "        \n",
    "        return pos_encoding\n",
    "    \n",
    "    def multi_head_attention_block(self, inputs, name_prefix=\"attention\"):\n",
    "        \"\"\"Multi-head attention block with residual connection\"\"\"\n",
    "        # Multi-head self-attention\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=self.n_heads,\n",
    "            key_dim=self.d_model // self.n_heads,\n",
    "            dropout=self.dropout_rate,\n",
    "            name=f\"{name_prefix}_mha\"\n",
    "        )(inputs, inputs)\n",
    "        \n",
    "        # Add & Norm\n",
    "        attention_output = layers.Dropout(self.dropout_rate)(attention_output)\n",
    "        attention_output = layers.Add(name=f\"{name_prefix}_add1\")([inputs, attention_output])\n",
    "        attention_output = layers.LayerNormalization(name=f\"{name_prefix}_norm1\")(attention_output)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_output = layers.Dense(self.d_model * 4, activation='relu', name=f\"{name_prefix}_ffn1\")(attention_output)\n",
    "        ffn_output = layers.Dropout(self.dropout_rate)(ffn_output)\n",
    "        ffn_output = layers.Dense(self.d_model, name=f\"{name_prefix}_ffn2\")(ffn_output)\n",
    "        \n",
    "        # Add & Norm\n",
    "        ffn_output = layers.Dropout(self.dropout_rate)(ffn_output)\n",
    "        ffn_output = layers.Add(name=f\"{name_prefix}_add2\")([attention_output, ffn_output])\n",
    "        output = layers.LayerNormalization(name=f\"{name_prefix}_norm2\")(ffn_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def build_classification_model(self, task_name=\"savings_prediction\"):\n",
    "        \"\"\"Build attention model for binary classification\"\"\"\n",
    "        # Input layer - treating each feature as a sequence element\n",
    "        inputs = layers.Input(shape=(self.n_features,), name=\"financial_features\")\n",
    "        \n",
    "        # Reshape for attention mechanism (batch_size, sequence_length=1, features)\n",
    "        x = layers.Reshape((1, self.n_features))(inputs)\n",
    "        \n",
    "        # Project to d_model dimensions\n",
    "        x = layers.Dense(self.d_model, name=\"input_projection\")(x)\n",
    "        \n",
    "        # Add positional encoding (even though sequence length is 1, useful for extensibility)\n",
    "        pos_encoding = self.positional_encoding(1, self.d_model)\n",
    "        x = x + pos_encoding\n",
    "        \n",
    "        # Multiple attention blocks\n",
    "        x = self.multi_head_attention_block(x, \"attention_block_1\")\n",
    "        x = self.multi_head_attention_block(x, \"attention_block_2\")\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        # Final classification layers\n",
    "        x = layers.Dense(256, activation='relu', name=\"dense_1\")(x)\n",
    "        x = layers.Dropout(self.dropout_rate)(x)\n",
    "        x = layers.Dense(128, activation='relu', name=\"dense_2\")(x)\n",
    "        x = layers.Dropout(self.dropout_rate)(x)\n",
    "        x = layers.Dense(64, activation='relu', name=\"dense_3\")(x)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = layers.Dense(1, activation='sigmoid', name=task_name)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs, name=f\"financial_{task_name}_model\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_multi_task_model(self):\n",
    "        \"\"\"Build multi-task attention model for multiple predictions\"\"\"\n",
    "        # Input layer\n",
    "        inputs = layers.Input(shape=(self.n_features,), name=\"financial_features\")\n",
    "        \n",
    "        # Reshape for attention mechanism\n",
    "        x = layers.Reshape((1, self.n_features))(inputs)\n",
    "        \n",
    "        # Project to d_model dimensions\n",
    "        x = layers.Dense(self.d_model, name=\"input_projection\")(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pos_encoding = self.positional_encoding(1, self.d_model)\n",
    "        x = x + pos_encoding\n",
    "        \n",
    "        # Shared attention blocks\n",
    "        shared_features = self.multi_head_attention_block(x, \"shared_attention_1\")\n",
    "        shared_features = self.multi_head_attention_block(shared_features, \"shared_attention_2\")\n",
    "        \n",
    "        # Global pooling\n",
    "        pooled_features = layers.GlobalAveragePooling1D()(shared_features)\n",
    "        \n",
    "        # Task-specific branches\n",
    "        # 1. Savings Achievement Classification\n",
    "        savings_branch = layers.Dense(128, activation='relu', name=\"savings_dense_1\")(pooled_features)\n",
    "        savings_branch = layers.Dropout(self.dropout_rate)(savings_branch)\n",
    "        savings_branch = layers.Dense(64, activation='relu', name=\"savings_dense_2\")(savings_branch)\n",
    "        savings_output = layers.Dense(1, activation='sigmoid', name=\"can_achieve_savings\")(savings_branch)\n",
    "        \n",
    "        # 2. Savings Amount Regression\n",
    "        amount_branch = layers.Dense(128, activation='relu', name=\"amount_dense_1\")(pooled_features)\n",
    "        amount_branch = layers.Dropout(self.dropout_rate)(amount_branch)\n",
    "        amount_branch = layers.Dense(64, activation='relu', name=\"amount_dense_2\")(amount_branch)\n",
    "        amount_output = layers.Dense(1, name=\"savings_amount\")(amount_branch)\n",
    "        \n",
    "        # 3. Financial Risk Classification\n",
    "        risk_branch = layers.Dense(128, activation='relu', name=\"risk_dense_1\")(pooled_features)\n",
    "        risk_branch = layers.Dropout(self.dropout_rate)(risk_branch)\n",
    "        risk_branch = layers.Dense(64, activation='relu', name=\"risk_dense_2\")(risk_branch)\n",
    "        risk_output = layers.Dense(1, activation='sigmoid', name=\"financial_risk\")(risk_branch)\n",
    "        \n",
    "        model = Model(\n",
    "            inputs=inputs, \n",
    "            outputs=[savings_output, amount_output, risk_output],\n",
    "            name=\"financial_multi_task_model\"\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_regression_model(self, task_name=\"savings_amount\"):\n",
    "        \"\"\"Build attention model for regression tasks\"\"\"\n",
    "        inputs = layers.Input(shape=(self.n_features,), name=\"financial_features\")\n",
    "        \n",
    "        x = layers.Reshape((1, self.n_features))(inputs)\n",
    "        x = layers.Dense(self.d_model, name=\"input_projection\")(x)\n",
    "        \n",
    "        pos_encoding = self.positional_encoding(1, self.d_model)\n",
    "        x = x + pos_encoding\n",
    "        \n",
    "        x = self.multi_head_attention_block(x, \"attention_block_1\")\n",
    "        x = self.multi_head_attention_block(x, \"attention_block_2\")\n",
    "        \n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        x = layers.Dense(256, activation='relu', name=\"dense_1\")(x)\n",
    "        x = layers.Dropout(self.dropout_rate)(x)\n",
    "        x = layers.Dense(128, activation='relu', name=\"dense_2\")(x)\n",
    "        x = layers.Dropout(self.dropout_rate)(x)\n",
    "        x = layers.Dense(64, activation='relu', name=\"dense_3\")(x)\n",
    "        \n",
    "        outputs = layers.Dense(1, name=task_name)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs, name=f\"financial_{task_name}_model\")\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1810a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(processed_data, feature_info):\n",
    "    \"\"\"Prepare data for training attention models\"\"\"\n",
    "    \n",
    "    # Get feature columns (exclude target variables)\n",
    "    feature_columns = feature_info['numerical_features'] + feature_info['categorical_features']\n",
    "    \n",
    "    # Prepare features\n",
    "    X = processed_data[feature_columns].values\n",
    "    \n",
    "    # Prepare targets\n",
    "    targets = {}\n",
    "    if 'Can_Achieve_Savings' in processed_data.columns:\n",
    "        targets['can_achieve_savings'] = processed_data['Can_Achieve_Savings'].values\n",
    "    if 'Desired_Savings' in processed_data.columns:\n",
    "        targets['savings_amount'] = processed_data['Desired_Savings'].values\n",
    "    if 'Savings_Gap' in processed_data.columns:\n",
    "        # Create financial risk based on savings gap\n",
    "        targets['financial_risk'] = (processed_data['Savings_Gap'] > processed_data['Savings_Gap'].quantile(0.7)).astype(int).values\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Available targets: {list(targets.keys())}\")\n",
    "    for name, target in targets.items():\n",
    "        print(f\"{name} distribution: {np.bincount(target) if target.dtype == int else 'continuous'}\")\n",
    "    \n",
    "    return X, targets, feature_columns\n",
    "\n",
    "# Prepare the data\n",
    "X, targets, feature_columns = prepare_data_for_training(processed_data, feature_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the attention model builder\n",
    "n_features = len(feature_columns)\n",
    "attention_builder = FinancialAttentionModel(\n",
    "    n_features=n_features,\n",
    "    n_heads=8,\n",
    "    d_model=128,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "print(f\"Building attention models for {n_features} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build and train Savings Achievement Classification Model\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING SAVINGS ACHIEVEMENT MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Build model\n",
    "savings_model = attention_builder.build_classification_model(\"savings_achievement\")\n",
    "savings_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "print(savings_model.summary())\n",
    "\n",
    "# Prepare data\n",
    "y_savings = targets['can_achieve_savings']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_savings, test_size=0.2, random_state=42, stratify=y_savings)\n",
    "\n",
    "# Training callbacks\n",
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
    "    callbacks.ModelCheckpoint('trained_model/best_savings_model.keras', save_best_only=True, monitor='val_accuracy')\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history_savings = savings_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = (savings_model.predict(X_test) > 0.5).astype(int)\n",
    "print(\"\\nSavings Achievement Classification Results:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build and train Savings Amount Regression Model\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING SAVINGS AMOUNT REGRESSION MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Build regression model\n",
    "amount_model = attention_builder.build_regression_model(\"savings_amount\")\n",
    "amount_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# Prepare data\n",
    "y_amount = targets['savings_amount']\n",
    "X_train_amt, X_test_amt, y_train_amt, y_test_amt = train_test_split(X, y_amount, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training callbacks\n",
    "callbacks_regression = [\n",
    "    callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
    "    callbacks.ModelCheckpoint('trained_model/best_amount_model.keras', save_best_only=True, monitor='val_mae')\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history_amount = amount_model.fit(\n",
    "    X_train_amt, y_train_amt,\n",
    "    validation_data=(X_test_amt, y_test_amt),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks_regression,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_amt = amount_model.predict(X_test_amt)\n",
    "mae = np.mean(np.abs(y_test_amt - y_pred_amt.flatten()))\n",
    "mse = np.mean((y_test_amt - y_pred_amt.flatten()) ** 2)\n",
    "print(f\"\\nSavings Amount Regression Results:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mse):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build and train Multi-task Model - FIXED VERSION\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING MULTI-TASK MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Build multi-task model\n",
    "multi_task_model = attention_builder.build_multi_task_model()\n",
    "multi_task_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'can_achieve_savings': 'binary_crossentropy',\n",
    "        'savings_amount': 'mse',\n",
    "        'financial_risk': 'binary_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'can_achieve_savings': 1.0,\n",
    "        'savings_amount': 0.001,  # Scale down due to different magnitude\n",
    "        'financial_risk': 1.0\n",
    "    },\n",
    "    metrics={\n",
    "        'can_achieve_savings': ['accuracy'],\n",
    "        'savings_amount': ['mae'],\n",
    "        'financial_risk': ['accuracy']\n",
    "    }\n",
    ")\n",
    "\n",
    "print(multi_task_model.summary())\n",
    "\n",
    "# Prepare multi-task data - FIXED APPROACH\n",
    "# Create combined target array for splitting\n",
    "y_combined = np.column_stack([\n",
    "    targets['can_achieve_savings'],\n",
    "    targets['savings_amount'],\n",
    "    targets['financial_risk']\n",
    "])\n",
    "\n",
    "# Split the data\n",
    "X_train_multi, X_test_multi, y_train_combined, y_test_combined = train_test_split(\n",
    "    X, y_combined, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert back to dictionary format for multi-output training\n",
    "y_train_multi_dict = {\n",
    "    'can_achieve_savings': y_train_combined[:, 0],\n",
    "    'savings_amount': y_train_combined[:, 1],\n",
    "    'financial_risk': y_train_combined[:, 2]\n",
    "}\n",
    "\n",
    "y_test_multi_dict = {\n",
    "    'can_achieve_savings': y_test_combined[:, 0],\n",
    "    'savings_amount': y_test_combined[:, 1],\n",
    "    'financial_risk': y_test_combined[:, 2]\n",
    "}\n",
    "\n",
    "print(f\"Multi-task training data shapes:\")\n",
    "print(f\"X_train_multi: {X_train_multi.shape}\")\n",
    "print(f\"y_train shapes: {[y_train_multi_dict[key].shape for key in y_train_multi_dict.keys()]}\")\n",
    "\n",
    "# Training callbacks\n",
    "callbacks_multi = [\n",
    "    callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6),\n",
    "    callbacks.ModelCheckpoint('trained_model/best_multi_task_model.keras', save_best_only=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "# Train multi-task model\n",
    "history_multi = multi_task_model.fit(\n",
    "    X_train_multi, y_train_multi_dict,\n",
    "    validation_data=(X_test_multi, y_test_multi_dict),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks_multi,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate multi-task model\n",
    "print(\"\\nMulti-task Model Evaluation:\")\n",
    "predictions_multi = multi_task_model.predict(X_test_multi)\n",
    "\n",
    "# Evaluate each task\n",
    "# 1. Savings Achievement Classification\n",
    "savings_pred = (predictions_multi[0] > 0.5).astype(int).flatten()\n",
    "print(\"\\nSavings Achievement Task:\")\n",
    "print(classification_report(y_test_combined[:, 0], savings_pred))\n",
    "\n",
    "# 2. Savings Amount Regression\n",
    "amount_pred = predictions_multi[1].flatten()\n",
    "mae_multi = np.mean(np.abs(y_test_combined[:, 1] - amount_pred))\n",
    "mse_multi = np.mean((y_test_combined[:, 1] - amount_pred) ** 2)\n",
    "print(f\"\\nSavings Amount Task:\")\n",
    "print(f\"MAE: {mae_multi:.2f}\")\n",
    "print(f\"MSE: {mse_multi:.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mse_multi):.2f}\")\n",
    "\n",
    "# 3. Financial Risk Classification\n",
    "risk_pred = (predictions_multi[2] > 0.5).astype(int).flatten()\n",
    "print(\"\\nFinancial Risk Task:\")\n",
    "print(classification_report(y_test_combined[:, 2], risk_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and Analysis\n",
    "def plot_training_history(history, title, save_name=None):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0].set_title(f'{title} - Loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot main metric\n",
    "    metric_keys = [k for k in history.history.keys() if 'accuracy' in k or 'mae' in k]\n",
    "    if metric_keys:\n",
    "        main_metric = metric_keys[0]\n",
    "        val_metric = f'val_{main_metric}'\n",
    "        if val_metric in history.history:\n",
    "            axes[1].plot(history.history[main_metric], label=f'Training {main_metric}')\n",
    "            axes[1].plot(history.history[val_metric], label=f'Validation {main_metric}')\n",
    "            axes[1].set_title(f'{title} - {main_metric}')\n",
    "            axes[1].set_xlabel('Epochs')\n",
    "            axes[1].set_ylabel(main_metric)\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_name:\n",
    "        plt.savefig(f'{save_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot training histories\n",
    "plot_training_history(history_savings, \"Savings Achievement Model\", \"savings_training\")\n",
    "plot_training_history(history_amount, \"Savings Amount Model\", \"amount_training\")\n",
    "plot_training_history(history_multi, \"Multi-Task Model\", \"multi_task_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Feature Importance Analysis\n",
    "def analyze_attention_weights(model, X_sample, feature_names, sample_idx=0):\n",
    "    \"\"\"Analyze attention weights for interpretability\"\"\"\n",
    "    # Get attention layer outputs\n",
    "    attention_layer = None\n",
    "    for layer in model.layers:\n",
    "        if 'attention' in layer.name and 'mha' in layer.name:\n",
    "            attention_layer = layer\n",
    "            break\n",
    "    \n",
    "    if attention_layer:\n",
    "        # Create a model that outputs attention weights\n",
    "        attention_model = Model(\n",
    "            inputs=model.input,\n",
    "            outputs=attention_layer.output\n",
    "        )\n",
    "        \n",
    "        # Get attention weights for a sample\n",
    "        sample = X_sample[sample_idx:sample_idx+1]\n",
    "        attention_output = attention_model.predict(sample)\n",
    "        \n",
    "        return attention_output\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Save models and create model summary\n",
    "model_summary = {\n",
    "    \"models_created\": {\n",
    "        \"savings_achievement_model\": {\n",
    "            \"type\": \"binary_classification\",\n",
    "            \"architecture\": \"attention\",\n",
    "            \"input_features\": n_features,\n",
    "            \"saved_as\": \"trained_model/best_savings_model.keras\"\n",
    "        },\n",
    "        \"savings_amount_model\": {\n",
    "            \"type\": \"regression\", \n",
    "            \"architecture\": \"attention\",\n",
    "            \"input_features\": n_features,\n",
    "            \"saved_as\": \"trained_model/best_amount_model.keras\"\n",
    "        },\n",
    "        \"multi_task_model\": {\n",
    "            \"type\": \"multi_task\",\n",
    "            \"tasks\": [\"can_achieve_savings\", \"savings_amount\", \"financial_risk\"],\n",
    "            \"architecture\": \"attention\",\n",
    "            \"input_features\": n_features,\n",
    "            \"saved_as\": \"trained_model/best_multi_task_model.keras\"\n",
    "        }\n",
    "    },\n",
    "    \"feature_info\": {\n",
    "        \"total_features\": n_features,\n",
    "        \"feature_names\": feature_columns,\n",
    "        \"numerical_features\": len(feature_info['numerical_features']),\n",
    "        \"categorical_features\": len(feature_info['categorical_features'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save model summary\n",
    "with open('../model/attention_models_summary.json', 'w') as f:\n",
    "    json.dump(model_summary, f, indent=2)\n",
    "\n",
    "print(\"Attention models training complete!\")\n",
    "print(f\"Models saved:\")\n",
    "print(\"1. trained_model/best_savings_model.keras - Savings achievement classification\")\n",
    "print(\"2. trained_model/best_amount_model.keras - Savings amount regression\") \n",
    "print(\"3. trained_model/best_multi_task_model.keras - Multi-task model\")\n",
    "print(\"4. ../model/attention_models_summary.json - Model metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e54166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models in multiple formats...\n",
      "Error in prediction pipeline: name 'tf' is not defined\n",
      "Models are trained but prediction pipeline needs adjustment\n"
     ]
    }
   ],
   "source": [
    "# Create prediction functions for real-world usage - FIXED VERSION\n",
    "def create_prediction_pipeline():\n",
    "    \"\"\"Create easy-to-use prediction functions\"\"\"\n",
    "    \n",
    "    # Define custom objects that might be needed for loading\n",
    "    custom_objects_for_load = {'Cast': tf.cast}\n",
    "    \n",
    "    # Local variables for loaded models\n",
    "    loaded_savings_model = None\n",
    "    loaded_amount_model = None\n",
    "    loaded_multi_task_model = None\n",
    "\n",
    "    try:\n",
    "        # Try to load SavedModel format first\n",
    "        try:\n",
    "            print(\"Attempting to load models from SavedModel format...\")\n",
    "            loaded_savings_model = tf.keras.models.load_model('trained_model/best_savings_model', custom_objects=custom_objects_for_load, compile=False)\n",
    "            loaded_amount_model = tf.keras.models.load_model('trained_model/best_amount_model', custom_objects=custom_objects_for_load, compile=False)\n",
    "            loaded_multi_task_model = tf.keras.models.load_model('trained_model/best_multi_task_model', custom_objects=custom_objects_for_load, compile=False)\n",
    "            print(\"Loaded models from SavedModel format.\")\n",
    "        except Exception as e_sm:\n",
    "            print(f\"Failed to load from SavedModel format: {e_sm}\")\n",
    "            print(\"Attempting to load models from .keras format...\")\n",
    "            # Fallback to .keras format with custom objects\n",
    "            loaded_savings_model = tf.keras.models.load_model('trained_model/best_savings_model.keras', \n",
    "                                                     custom_objects=custom_objects_for_load, \n",
    "                                                     compile=False)\n",
    "            loaded_amount_model = tf.keras.models.load_model('trained_model/best_amount_model.keras', \n",
    "                                                    custom_objects=custom_objects_for_load, \n",
    "                                                    compile=False)\n",
    "            loaded_multi_task_model = tf.keras.models.load_model('trained_model/best_multi_task_model.keras', \n",
    "                                                        custom_objects=custom_objects_for_load, \n",
    "                                                        compile=False)\n",
    "            print(\"Loaded models from .keras format.\")\n",
    "    except Exception as e_load_all:\n",
    "        print(f\"Could not load saved models: {e_load_all}\")\n",
    "        print(\"Using models from current session (if they were trained and are globally available)...\")\n",
    "        # If loading fails, the predict functions will rely on Python's scoping rules\n",
    "        # to find savings_model, amount_model, multi_task_model in the global scope\n",
    "        # (assuming they were trained earlier in the notebook).\n",
    "        # To make this explicit for the predict functions defined below,\n",
    "        # we assign the global models to the local variables.\n",
    "        # This assumes the global variables have these exact names.\n",
    "        global savings_model, amount_model, multi_task_model\n",
    "        loaded_savings_model = savings_model\n",
    "        loaded_amount_model = amount_model\n",
    "        loaded_multi_task_model = multi_task_model\n",
    "        \n",
    "    # Assign to the names expected by the inner prediction functions\n",
    "    # These will be captured by the closures of the prediction functions\n",
    "    # This step is crucial: make the successfully loaded (or fallback global) models\n",
    "    # available to the predict_... functions under the names they expect.\n",
    "    savings_model_to_use = loaded_savings_model\n",
    "    amount_model_to_use = loaded_amount_model\n",
    "    multi_task_model_to_use = loaded_multi_task_model\n",
    "\n",
    "    if None in [savings_model_to_use, amount_model_to_use, multi_task_model_to_use]:\n",
    "        print(\"Warning: One or more models could not be loaded and are not available from the global session. Prediction functions may fail.\")\n",
    "        # Optionally, raise an error here if models are essential:\n",
    "        # raise RuntimeError(\"Failed to initialize prediction pipeline: Models not available.\")\n",
    "\n",
    "    def predict_savings_achievement(features):\n",
    "        \"\"\"Predict if user can achieve their savings goal\"\"\"\n",
    "        if savings_model_to_use is None:\n",
    "            raise RuntimeError(\"Savings achievement model is not available.\")\n",
    "        prediction = savings_model_to_use.predict(features.reshape(1, -1), verbose=0)\n",
    "        return {\n",
    "            'can_achieve_savings': bool(prediction[0][0] > 0.5),\n",
    "            'confidence': float(prediction[0][0])\n",
    "        }\n",
    "    \n",
    "    def predict_optimal_savings_amount(features):\n",
    "        \"\"\"Predict optimal savings amount\"\"\"\n",
    "        if amount_model_to_use is None:\n",
    "            raise RuntimeError(\"Savings amount model is not available.\")\n",
    "        prediction = amount_model_to_use.predict(features.reshape(1, -1), verbose=0)\n",
    "        return {\n",
    "            'recommended_savings': float(prediction[0][0])\n",
    "        }\n",
    "    \n",
    "    def predict_comprehensive_financial_profile(features):\n",
    "        \"\"\"Get comprehensive financial predictions\"\"\"\n",
    "        if multi_task_model_to_use is None:\n",
    "            raise RuntimeError(\"Multi-task model is not available.\")\n",
    "        predictions = multi_task_model_to_use.predict(features.reshape(1, -1), verbose=0)\n",
    "        return {\n",
    "            'can_achieve_savings': bool(predictions[0][0][0] > 0.5),\n",
    "            'savings_confidence': float(predictions[0][0][0]),\n",
    "            'recommended_savings_amount': float(predictions[1][0][0]),\n",
    "            'financial_risk': bool(predictions[2][0][0] > 0.5),\n",
    "            'risk_score': float(predictions[2][0][0])\n",
    "        }\n",
    "    \n",
    "    return predict_savings_achievement, predict_optimal_savings_amount, predict_comprehensive_financial_profile\n",
    "\n",
    "# Save models in both formats for better compatibility\n",
    "print(\"Saving models in multiple formats...\")\n",
    "\n",
    "# Save in SavedModel format (recommended)\n",
    "try:\n",
    "    predict_savings, predict_amount, predict_comprehensive = create_prediction_pipeline()\n",
    "    \n",
    "    # Test with a sample\n",
    "    sample_features = X_test[0]\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(\"Savings Achievement:\", predict_savings(sample_features))\n",
    "    print(\"Optimal Amount:\", predict_amount(sample_features))\n",
    "    print(\"Comprehensive Profile:\", predict_comprehensive(sample_features))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ATTENTION MODELS READY FOR DEPLOYMENT!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in prediction pipeline: {e}\")\n",
    "    print(\"Models are trained but prediction pipeline needs adjustment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dad00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def test_saved_models(sample_features):\n",
    "    \"\"\"Test all three saved models with a sample feature vector.\"\"\"\n",
    "    # Load models (try .keras format for simplicity)\n",
    "    savings_model = tf.keras.models.load_model('trained_model/best_savings_model.keras', compile=False)\n",
    "    amount_model = tf.keras.models.load_model('trained_model/best_amount_model.keras', compile=False)\n",
    "    multi_task_model = tf.keras.models.load_model('trained_model/best_multi_task_model.keras', compile=False)\n",
    "\n",
    "    # Ensure input is numpy array and correct shape\n",
    "    features = np.array(sample_features).reshape(1, -1)\n",
    "\n",
    "    # 1. Savings Achievement Prediction\n",
    "    pred_savings = savings_model.predict(features, verbose=0)\n",
    "    savings_result = {\n",
    "        'can_achieve_savings': bool(pred_savings[0][0] > 0.5),\n",
    "        'confidence': float(pred_savings[0][0])\n",
    "    }\n",
    "\n",
    "    # 2. Savings Amount Prediction\n",
    "    pred_amount = amount_model.predict(features, verbose=0)\n",
    "    amount_result = {\n",
    "        'recommended_savings': float(pred_amount[0][0])\n",
    "    }\n",
    "\n",
    "    # 3. Multi-task Model Prediction\n",
    "    pred_multi = multi_task_model.predict(features, verbose=0)\n",
    "    multi_result = {\n",
    "        'can_achieve_savings': bool(pred_multi[0][0][0] > 0.5),\n",
    "        'savings_confidence': float(pred_multi[0][0][0]),\n",
    "        'recommended_savings_amount': float(pred_multi[1][0][0]),\n",
    "        'financial_risk': bool(pred_multi[2][0][0] > 0.5),\n",
    "        'risk_score': float(pred_multi[2][0][0])\n",
    "    }\n",
    "\n",
    "    print(\"Test Results for Saved Models:\")\n",
    "    print(\"Savings Achievement:\", savings_result)\n",
    "    print(\"Optimal Amount:\", amount_result)\n",
    "    print(\"Comprehensive Profile:\", multi_result)\n",
    "    return savings_result, amount_result, multi_result\n",
    "\n",
    "# Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e01c84bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 129 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001BA6841ECA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 129 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001BA6841ECA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 130 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001BA6841FCE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 130 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001BA6841FCE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for Saved Models:\n",
      "Savings Achievement: {'can_achieve_savings': True, 'confidence': 0.9999999403953552}\n",
      "Optimal Amount: {'recommended_savings': 843.02392578125}\n",
      "Comprehensive Profile: {'can_achieve_savings': True, 'savings_confidence': 0.9953286051750183, 'recommended_savings_amount': 1025.1810302734375, 'financial_risk': True, 'risk_score': 0.5527254343032837}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'can_achieve_savings': True, 'confidence': 0.9999999403953552},\n",
       " {'recommended_savings': 843.02392578125},\n",
       " {'can_achieve_savings': True,\n",
       "  'savings_confidence': 0.9953286051750183,\n",
       "  'recommended_savings_amount': 1025.1810302734375,\n",
       "  'financial_risk': True,\n",
       "  'risk_score': 0.5527254343032837})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_saved_models(X_test[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
